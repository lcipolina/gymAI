{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imported: np, gym, random, time, plt, clear_output\n"
     ]
    }
   ],
   "source": [
    "#Import packages\n",
    "%run imports_gym.ipynb"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Q learning algorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class Qvalue:\n",
    "    '''\n",
    "    Implements Qvalue learning\n",
    "    '''    \n",
    "    def __init__(self, environment, num_episodes, max_steps_per_episode, learning_rate=0.1, discount_rate=0.99,exploration_rate =1, max_exploration_rate = 1, min_exploration_rate   = 0.01, exploration_decay_rate = 0.001):\n",
    "        self.env                    = gym.make(environment)\n",
    "        self.num_episodes           = num_episodes\n",
    "        self.max_steps_per_episode  = max_steps_per_episode \n",
    "        self.learning_rate          = learning_rate    \n",
    "        self.discount_rate          = discount_rate \n",
    "        self.exploration_rate       = exploration_rate\n",
    "        self.max_exploration_rate   = max_exploration_rate \n",
    "        self.min_exploration_rate   = min_exploration_rate\n",
    "        self.exploration_decay_rate = exploration_decay_rate        \n",
    "        self.env.reset() #start from the beginning of the episode\n",
    "        return     \n",
    "   \n",
    "    # HELPERS*************************************************************************************************\n",
    "    def policy(self,state,q_table):\n",
    "        # Exploration policy -Exploration-exploitation trade-off: decide whether we explore or exploit\n",
    "        exploration_rate_threshold = random.uniform(0, 1) #pick a rnd value and compare it to the set threshold\n",
    "        if exploration_rate_threshold > self.exploration_rate:\n",
    "            action = np.argmax(q_table[state,:]) #exploit and take the max\n",
    "        else:\n",
    "            action = self.env.action_space.sample() #explore - sample an action randomly-        \n",
    "        return action\n",
    "    \n",
    "    def updateQ(self,q_table,reward,action,state,new_state):\n",
    "        # Update Q-table for Q(s,a) - implementation of the Q-value according to the chosen \"leaerning rate\"\n",
    "        td_err =  (reward + self.discount_rate * np.max(q_table[new_state, :]))  \n",
    "        q_table[state, action] = q_table[state, action] * (1 - self.learning_rate) +self.learning_rate *td_err\n",
    "        return q_table\n",
    "    \n",
    "    \n",
    "    # MAIN *****************************************************************************************************\n",
    "    def run(self):\n",
    "        rewards_all_episodes = []  #list container to hold all the rewards across episodes\n",
    "        q_table = np.zeros((self.env.observation_space.n, self.env.action_space.n))#OBS: state_space_size  = env.observation_space.n\n",
    "            \n",
    "        for episode in range(self.num_episodes):\n",
    "            # initialize new episode params \n",
    "            state = self.env.reset() #put the agent back on the starting state every time\n",
    "            done  = False       #the 'done' variable keeps track of whether or not we have finished the episode (i.e. the game)\n",
    "            rewards_current_episode = 0 #keeps track of the accumulated rewards within the episode\n",
    "    \n",
    "            for step in range(self.max_steps_per_episode):             \n",
    "                #Agent\n",
    "                action  = self.policy(state,q_table)               \n",
    "                #Environment             \n",
    "                new_state, reward, done, info = self.env.step(action) #we store on tuples all the info output by the environment\n",
    "                #Q-value matrix\n",
    "                q_table = self.updateQ(q_table,reward,action,state,new_state) \n",
    "                #Set new state\n",
    "                state   = new_state\n",
    "                #Add new reward     \n",
    "                rewards_current_episode += reward         \n",
    "            # Check whether the last step has ended the game (we get this variable from the environment)\n",
    "            # If the action did end the episode, then we jump out of this loop and move on to the next episode.\n",
    "            # Otherwise, we transition to the next time-step within the same episode (i.e. the same game) \n",
    "                if done == True: \n",
    "                    break\n",
    "\n",
    "        ################### Once the steps are finished ###########################    \n",
    "        # Exploration rate decay    - this is just a trick to improve performance.\n",
    "        # We want to explore less and exploit more with time\n",
    "            self.exploration_rate =  self.min_exploration_rate +(self.max_exploration_rate - self.min_exploration_rate) * np.exp(-self.exploration_decay_rate*episode)\n",
    "                \n",
    "       # Add current episode reward to total rewards list\n",
    "            rewards_all_episodes.append(rewards_current_episode)\n",
    "        \n",
    "        return  rewards_all_episodes  \n",
    "    \n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Rider"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#env name ####\n",
    "environment = \"FrozenLake-v0\"\n",
    "\n",
    "#define static variables\n",
    "num_episodes           = 10000 #number of games to play\n",
    "max_steps_per_episode  = 100   #how many steps on each game\n",
    "learning_rate          = 0.1   #how much we want to update the Q-value at each stage.\n",
    "discount_rate          = 0.99  #how much weight we give to future rewards\n",
    "\n",
    "#### experimental values ####\n",
    "exploration_rate       = 1\n",
    "max_exploration_rate   = 1\n",
    "min_exploration_rate   = 0.01\n",
    "exploration_decay_rate = 0.001  #this is experimental.. change and see what happens.\n",
    "\n",
    "qval                 = Qvalue(environment, num_episodes, max_steps_per_episode, learning_rate, discount_rate,exploration_rate, max_exploration_rate, min_exploration_rate, exploration_decay_rate)\n",
    "rewards_all_episodes = qval.run()\n",
    "\n",
    "#print(rewards_all_episodes)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "********Average reward per thousand episodes (in percentage)********\n",
      "\n",
      "1000 :  0.047000000000000035\n",
      "2000 :  0.19900000000000015\n",
      "3000 :  0.4080000000000003\n",
      "4000 :  0.5450000000000004\n",
      "5000 :  0.6330000000000005\n",
      "6000 :  0.6800000000000005\n",
      "7000 :  0.6720000000000005\n",
      "8000 :  0.6660000000000005\n",
      "9000 :  0.6890000000000005\n",
      "10000 :  0.6940000000000005\n"
     ]
    }
   ],
   "source": [
    "# Calculate and print the average reward per thousand episodes\n",
    "\n",
    "rewards_per_thosand_episodes = np.split(np.array(rewards_all_episodes),num_episodes/1000)\n",
    "count = 1000\n",
    "print(\"********Average reward per thousand episodes (in percentage)********\\n\") ## we want the reward to improve over time\n",
    "for r in rewards_per_thosand_episodes:\n",
    "    print(count, \": \", str(sum(r/1000)))\n",
    "    count += 1000 \n",
    "    \n",
    "\n",
    "# Print updated Q-table\n",
    "#print(\"\\n\\n********Q-table********\\n\")\n",
    "#print(q_table)  ### TODO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:gym] *",
   "language": "python",
   "name": "conda-env-gym-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}